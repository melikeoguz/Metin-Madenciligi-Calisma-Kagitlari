{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">DOĞAL DİL İŞLEME (NLP)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doğal dil işlemeyi Python'da yer alan <b>NLTK (Natural Language Tool Kit) Kütüphanesi</b> yardımıyla gerçekleştirmekteyiz.\n",
    "Bunun için öncelikle sistemimize ntlk kütüphanesini indirmeliyiz. Ben, işlemlerimi jupyter notebook üzerinden \n",
    "gerçekleştirdiğimden ötürü bu kütüphanenin indirilmesini terminal üzerinden gerçekleştirdim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ins>NTLK Kütüphanesi Nasıl İndirilir?</ins></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\n",
    "<li>Öncelikle bilgisayarınızda python'ın yüklediğiniz klasörün içine girin.</li>\n",
    "<li>Daha sonra <b>Scripts</b> adlı klasöre tıklayınız.</li>\n",
    "<li>Git Bash terminalini burada çalıştırınız.</li>\n",
    "<li>Daha sonra <code>pip3 install ntlk</code> komut satırını terminale yazınız</li>\n",
    "<li>Eğer işlemleriniz doğru ise <i>Successfully installed click-7.1.2 joblib-0.16.0 nltk-3.5 regex-2020.7.14 tqdm-4.48.2\n",
    "</i> yazısını terminalizden görünteleyebilirsiniz</li>\n",
    "\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tüm adımları aşağıdaki görseli inceleyerek gerçekleştirebilirsiniz !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/images/nltk-indirmek.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.book import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ins>NLTK Paketlerinin İndirilmesi</ins></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıda yazan cümlelerin otomatik olarak çalışma kağıdına gelmesi için <code>nltk.download()</code> kodunun çalıştırılması\n",
    "gerekmektedir. Bu kod ile bilgisayarınıza aşağıdaki gibi <b>NLTK Downloader</b> uygulaması açılacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/images/nltk-downloader.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yukarıdaki ekrandan <b>all seçeneğine</b> tıklayın. Bu paketler ile birlikte text corpora, wordnet gibi birçok paket \n",
    "bilgisayarınıza kurulmuş olacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tüm işlem tamamlandığında aşağıdaki gibi bir ekran karşınıza gelecektir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/images/all-packages.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bu adımdan sonra **jupyter notebook'unuzu** açın.Örnek textleriniz ekrana geldiğini göreceksiniz. İşlemlerinizi yapmaya devam edebilirsiniz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">NTLK Kütüphanesi ile İşlemler</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7 #text7'yi goruntule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7 #cumleyi goruntule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7) #kelime sayisini bul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7) #text7'deki karakter sayisini bul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text7)) #text7'deki unique degerlerin karakter sayisini bul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Donuts',\n",
       " 'shot',\n",
       " 'Bunny',\n",
       " 'keyboards',\n",
       " 'Fears',\n",
       " 'Labor',\n",
       " 'sweet',\n",
       " 'fellow',\n",
       " 'foundering',\n",
       " 'quantitive']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(text7))[:10] #text7'deki ilk 10 unique kelimeyi listele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ins>Kelimelerin Frekanslarını (Sıklıklarını) Bulma</ins></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelimelerin frekanslarını hesaplamak için NLTK kütüphanesinde yer alan <b>FreqDist</b> metodunu kullanırız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 4885, 'the': 4045, '.': 3828, 'of': 2319, 'to': 2164, 'a': 1878, 'in': 1572, 'and': 1511, '*-1': 1123, '0': 1099, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = FreqDist(text7) #text7 icerisindeki kelimelerin frekanslarını hesapla\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Yukarıda gördüğünüz örneğe göre;</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<li>4885 adet ,</li>\n",
    "<li>4045 adet the</li>\n",
    "<li>3828 adet .</li>\n",
    "<li>2319 adet of</li>\n",
    "<li>2164 adet to</li>\n",
    "<li>1878 adet a</li>\n",
    "<li>1572 adet in</li>\n",
    "<li>1511 adet and kullanılmıştır.</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = dist.keys() #liste şeklinde unique kelimeleri vocab1 degiskenine ata\n",
    "#vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab1)[:10] #ilk 10 kelimeyi listele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ins>İstenilen Kelimenin Frekansını Bulma<ins></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunun için NTLK kütüphanesinde yer alan <b>dist metodunu</b> kullanmamız gerekmektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['four'] #four kelimesinin frekansını bulma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100] #5 karakterden uzun frekansı 100'den fazla olan kelimeleri yaz\n",
    "freqwords  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Normalleştirme ve Kök Bulma (Normalization and Stemming)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cümlelerin içinde kullanılan kelimeler her zaman yalın halde kullanılmaz. İngilizce üzerinden gidersek eğer, kelimeler \n",
    "kullanıldıkları zaman kavramına göre belli ekler almaktadır. Bu kelimelerin köklerini bulmak için öncelikle cümle normalleştirilip\n",
    "daha sonra kök bulma işlemine tâbi tutulur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Köklerin bulunması için öncelikle tüm kelimelerin küçük harf karakteriyle başlaması gerekmektedir. Cümle başlangıcında yer alan \n",
    "kelimeler genellikle büyük harfle başladığı için öncelikle aşağıdaki gibi tüm cümle küçük harfe çevrilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ') #tum cumleyi kucuk harfe cevir ve bosluk karakterine gore ayir\n",
    "words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çevirme işlemi tamamlandıktan sonra NLTK Kütüphanesinde yer alan <b>PorterStemmer metodu</b> ile tum kelimeler bir liste atılır.\n",
    "Daha sonra listenin adı.<b>stem</b> metodu ile tüm kelimelerin kökleri bulunur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1] #words1 listesindeki tum kelimelerin koklerini bul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Lemmatization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1') #corpusa kayıtlı english-latin1 olan kelimeleri udhr degiskenine ata\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers',\n",
       " 'declar',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preambl',\n",
       " 'wherea',\n",
       " 'recognit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inher',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalien',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in udhr[:20]] #udhr listesindeki ilk 20 kelimenin kökünü bul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Tokenization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Tokenizasyon, hassas verilerin rastgele ve benzersiz bir değer ile ifade edilmesine denilmektedir. NLP işlemlerinde kullanılma\n",
    "sebebi ise bazı metotların ihtiyacı tam karşılayamamasıdır.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Örneğin;</i> text11'e atadığımız cümleyi boşluk karakterine göre ayıralım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\" \n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğer cümleleri bu şekilde ayırıp kök bulma işlemine sokarsak verimli bir sonuç elde edemeyiz. Çünkü <b>shouldn't</b> kelimesi\n",
    "aslında <b>should</b> ve **not** kelimelerinin birleşmesi ile oluşmaktadır. Boşluk karakterine göre ayırdığımızda\n",
    "bu iki ayrı kelimeyi tek kelime olarak algılamaktadır. Bir diğer sorun ise cümle sonundaki noktalama işaretini son kelime ile\n",
    "birleşik tutması. O yüzden cümle içerisindeki bu tür kısaltmaları ayrı ayrı ele almamız gerekmektedir. Bunun için de\n",
    "**word_tokenize** metodunu kullanılır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ins>Metinleri Cümlelere Ayırmak</ins></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metinde yer alan cümleleri bir liste halinde tutmak için NLTK Kütüphanesinde yer alan **sent_tokenize** metodu kullanılmaktadır.\n",
    "Bu metodun çalışma mantığına göre metindeki cümleler .'lara göre ayrılmamaktadır. Eğer öyle olsaydı **U.S** kısaltması ve\n",
    "**$2.99** olarak ifade edilmiş para birimi biçim için sorun teşkil ederdi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text12) #text12'yi cumlelere ayir\n",
    "sentences                              #cumleleri ekrana bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) #sentences degiskeninin uzunlugunu (text12'deki cumle sayisini) ekrana bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
